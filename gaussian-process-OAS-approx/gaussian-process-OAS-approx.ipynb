{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "026d8e58",
   "metadata": {},
   "source": [
    "## Situation and Task\n",
    "- **Reason**: to provide realtime pricing of bonds with embedded options, prices need to be converted to OAS spread quickly, but the traditional OAS calculator is too slow.\n",
    "- **Difficulty**: OAS calculator is a high-dimensional input, complex function\n",
    "- **Existing solution**: pre-compute and cache OAS calc results, but those are expensive.\n",
    "- **Task**: build an OAS approximator with guaranteed precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc355bc8",
   "metadata": {},
   "source": [
    "## Action\n",
    "- **Initial solution**: Build a machine learning model (such as neural net), trained on the inputs $X$ and true output $y$ from the OAS calculator. The trained model can then be the approximator.\n",
    "- **Further difficulty**: expensive to generate training samples, especially when we need to guarantee precision. \n",
    "- **Refinement**: Made the connection in hyperparameter searching in neural network and solve this above problem adapting from Bayesian optimization/Gaussian process.\n",
    "\t- The problem is similar to **crude oil extraction**: we do not know where the oil is, and it is expensive to dig holes\n",
    "\t- So the solution is that we trial a few places, analyze the soil sample and determine which regions to dig next, and iterate. At the end, if you look at the actual holes we dig in the area, you will find that there are more holes in areas where there is high likelihood of oil.\n",
    "\t- The adapted Bayesian optimization for our problem is similarly an automatic way to search. \n",
    "        - As an **iterative process**, given the existing model errors on the current samples, we analyze and determine which region that model is more likely to do poorly (i.e. large model errors so far). Then we sample more in those regions and reevaluate.\n",
    "        - In this way, we **get away from having to sample too many unnecessary points** for training samples: since for areas where the OAS approximator is already doing a good job, we can afford to sample less, just as we can forget about areas in ground when we are highly certain there is no oil underneath."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6311b",
   "metadata": {},
   "source": [
    "## Resolution\n",
    "- **Achievement**: \n",
    "    - Machine learning solution + Bayesian optimization is able to **save 100x compute time and 1000x storage**\n",
    "    - Make viable a business proposal: it is a **0 to 1 breakthrough**\n",
    "- **Take-away**. It is also remarkable intellectually, as it showcase the **power of making conceptual connection and adapting solutions**\n",
    "\t- Machine learning algo were used to e.g. identify cats in the picture, but it is just approximating complex logics and functions, which makes it suitable for OAS approximation.\n",
    "\t- Bayesian optimization was traditionally used to find the maximum value of high-dimensional functions, but it can also be used as a systematic way of search, which is suitable to be adapted to sample training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f0e95a",
   "metadata": {},
   "source": [
    "## Technical Details\n",
    "\n",
    "### Input space - high dimensional\n",
    "\n",
    "- price, volatility, coupon, non-call in years, maturity, tenor values of the base curve\n",
    "\n",
    "- The goal is to approximate the OAS calculator with **guarantee of precision** in a range of the above inputs, \n",
    "    - where the range of price, volatility and curve indicate what **market environment** fits the calibration, \n",
    "    - while the range of coupon, non-call year and maturity indicate **which bonds can use the approximator**.\n",
    "\n",
    "- It is clear that a simple gridding and caching approach will quickly run afoul of the **curse of dimensionality**.\n",
    "\n",
    "- We also tried reducing the dimensionality of **base curve by using Nielson-Siegal components**, i.e. PCA\n",
    "\n",
    "### The machine learning model - MLP\n",
    "\n",
    "- We tried single and double-layered MLP, with a `tanh` layer in between.\n",
    "\n",
    "- **Number of neurons** is found to be sufficient to be 30-50. \n",
    "    - The small and simple network **does not require us to spend much time in hyperparameter turning**: empirically, **larger networks have bigger chance of having saddle points**\n",
    "\n",
    "- We find that `MLPRegressor` in `sklearn` with `lbfgs` as optimizer works reasonably well.\n",
    "\n",
    "- We specify **No regularization**, as the implicit assumption is the true OAS calculator is not noisy (when its precision is sufficiently high see below).\n",
    "\n",
    "- Loss function is defined to be **p mean**, where the **larger the p, the more we avert large error**. This is a way to guarantee precision, at least empirically.\n",
    "\n",
    "- The main headache is the **precision of the true OAS calculator** and to **sample smartly** (see the above point for Gaussian process).\n",
    "    - The two issues compound each other: to have better precision, it is more expensive to run the true OAS calculator, while it is all the more important to sub-sample that counts.\n",
    "\n",
    "\n",
    "### The Gaussian process optimization\n",
    "\n",
    "- In each iteration, train the MLP on the given set of data, obtain the errors\n",
    "- Train a Gaussian process on the data points of errors.\n",
    "- Further generate extra points in the input space, especially around points which are significantly non-zero. See which newly sample points are predicted to be significantly non-zero by the Gaussian process. Add those to the set of data to be re-trained by MLP next.\n",
    "    - We specify significance of zero as just absolute value of posterior mean over posterior standard deviation: not clear whether this will perform better or worse than the expected improvement, but it is there for simplicity.\n",
    "- Repeat, until there is no added data, or the number of data points reach a threshold\n",
    "\n",
    "#### Specifying the Gaussian process\n",
    "\n",
    "- The kernel is **isotropic**, i.e. only depends on the distance between the two input points. \n",
    "    - The **distance of input points are weighted Euclidean**, reflecting the belief which dimension the error is likely to be more sensitive on.\n",
    "    - To customize the above kernel, needed to **override the call method in kernel in `sklearn`**.\n",
    "\n",
    "- We **do not specify extra noise** (i.e. `alpha` is the defaulted very small value in the API). The implicit assumption is we trust the precision of the true OAS calculator we are approximating.\n",
    "\n",
    "#### Next steps of refining the Gaussian process\n",
    "\n",
    "- GP is probably reaching the **limit of number of data** (a few thousand) since we need to invert the covariance matrix in training\n",
    "\n",
    "- GP is probably reaching the **limit of the number of features** as well: should not be more than a dozen.\n",
    "\n",
    "- Maybe a better logic to judge the significance of errors on the next points are needed; see ‘expected improvement’ in Bayesian optimization: https://towardsdatascience.com/bayesian-optimization-a-step-by-step-approach-a1cb678dd2ec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
